{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3b0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (Input, Conv2D, BatchNormalization, Activation,\n",
    "                                     Dense, MaxPooling2D, Dropout, Flatten, \n",
    "                                     GlobalAveragePooling2D, Add)\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1a95e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'train_dir': 'FER/train',\n",
    "    'test_dir': 'FER/test',\n",
    "    'img_size': (48, 48),\n",
    "    'input_shape': (48, 48, 1),\n",
    "    'num_classes': 7,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 50,\n",
    "    'validation_split': 0.2,\n",
    "    'kernel_size': (3, 3),\n",
    "    'pool_size': (2, 2),\n",
    "    'use_batch_norm': True,\n",
    "    'dense_units': 128,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "hyperparameters = {\n",
    "    'filters_base': [32, 48, 64],\n",
    "    'dropout_conv': [0.3, 0.4, 0.5],\n",
    "    'dropout_dense': [0.3, 0.5],\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e78cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22943 images belonging to 7 classes.\n",
      "Found 5730 images belonging to 7 classes.\n",
      "Found 7169 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,           \n",
    "    width_shift_range=0.1,       \n",
    "    height_shift_range=0.1,      \n",
    "    horizontal_flip=True,        \n",
    "    zoom_range=0.1,              \n",
    "    fill_mode='nearest',\n",
    "    validation_split=CONFIG['validation_split']\n",
    ")\n",
    "\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    CONFIG['train_dir'], \n",
    "    target_size=CONFIG['img_size'], \n",
    "    color_mode='grayscale',\n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    class_mode='categorical',\n",
    "    shuffle=True, \n",
    "    subset='training', \n",
    "    seed=CONFIG['random_seed']\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    CONFIG['train_dir'], \n",
    "    target_size=CONFIG['img_size'], \n",
    "    color_mode='grayscale',\n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    class_mode='categorical',\n",
    "    shuffle=False, \n",
    "    subset='validation', \n",
    "    seed=CONFIG['random_seed']\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    CONFIG['test_dir'], \n",
    "    target_size=CONFIG['img_size'], \n",
    "    color_mode='grayscale',\n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    class_mode='categorical', \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0a9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e40de8",
   "metadata": {},
   "source": [
    "**2D CNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43dbc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_tuner(hp):\n",
    "    # Tune hyperparameters\n",
    "    filters_base = hp.Choice('filters_base', values=hyperparameters['filters_base'])\n",
    "    dropout_conv = hp.Choice('dropout_conv', values=hyperparameters['dropout_conv'])\n",
    "    dropout_dense = hp.Choice('dropout_dense', values=hyperparameters['dropout_dense'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=hyperparameters['learning_rate'])\n",
    "    \n",
    "    model = Sequential(name='Baseline_CNN')\n",
    "    model.add(Input(shape=CONFIG['input_shape'], name='input'))\n",
    "    \n",
    "    # Block 1\n",
    "    model.add(Conv2D(filters_base, CONFIG['kernel_size'], activation='relu', \n",
    "                     padding='same', name='conv1_1'))\n",
    "    if CONFIG['use_batch_norm']:\n",
    "        model.add(BatchNormalization(name='bn1_1'))\n",
    "    model.add(Conv2D(filters_base, CONFIG['kernel_size'], activation='relu', \n",
    "                     padding='same', name='conv1_2'))\n",
    "    if CONFIG['use_batch_norm']:\n",
    "        model.add(BatchNormalization(name='bn1_2'))\n",
    "    model.add(MaxPooling2D(pool_size=CONFIG['pool_size'], name='pool1'))\n",
    "    model.add(Dropout(dropout_conv, name='dropout1'))\n",
    "    \n",
    "    # Block 2\n",
    "    model.add(Conv2D(filters_base * 2, CONFIG['kernel_size'], activation='relu', \n",
    "                     padding='same', name='conv2_1'))\n",
    "    if CONFIG['use_batch_norm']:\n",
    "        model.add(BatchNormalization(name='bn2_1'))\n",
    "    model.add(Conv2D(filters_base * 2, CONFIG['kernel_size'], activation='relu', \n",
    "                     padding='same', name='conv2_2'))\n",
    "    if CONFIG['use_batch_norm']:\n",
    "        model.add(BatchNormalization(name='bn2_2'))\n",
    "    model.add(MaxPooling2D(pool_size=CONFIG['pool_size'], name='pool2'))\n",
    "    model.add(Dropout(dropout_conv, name='dropout2'))\n",
    "    \n",
    "    # Block 3\n",
    "    model.add(Conv2D(filters_base * 4, CONFIG['kernel_size'], activation='relu', \n",
    "                     padding='same', name='conv3'))\n",
    "    if CONFIG['use_batch_norm']:\n",
    "        model.add(BatchNormalization(name='bn3'))\n",
    "    model.add(MaxPooling2D(pool_size=CONFIG['pool_size'], name='pool3'))\n",
    "    model.add(Dropout(dropout_conv, name='dropout3'))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Flatten(name='flatten'))\n",
    "    model.add(Dense(CONFIG['dense_units'], activation='relu', name='dense1'))\n",
    "    if CONFIG['use_batch_norm']:\n",
    "        model.add(BatchNormalization(name='bn_dense'))\n",
    "    model.add(Dropout(dropout_dense, name='dropout_dense'))\n",
    "    \n",
    "    model.add(Dense(CONFIG['num_classes'], activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8b181",
   "metadata": {},
   "source": [
    "**ResNet18**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c92b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet18_tuner(hp):\n",
    "    filters_base = hp.Choice('filters_base', values=hyperparameters['filters_base'])\n",
    "    dropout_conv = hp.Choice('dropout_conv', values=hyperparameters['dropout_conv'])\n",
    "    dropout_dense = hp.Choice('dropout_dense', values=hyperparameters['dropout_dense'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=hyperparameters['learning_rate'])\n",
    "    \n",
    "    inputs = Input(shape=CONFIG['input_shape'], name='input')\n",
    "    \n",
    "    # Stem\n",
    "    x = Conv2D(filters_base, CONFIG['kernel_size'], strides=1, \n",
    "               padding='same', name='conv1')(inputs)\n",
    "    if CONFIG['use_batch_norm']:\n",
    "        x = BatchNormalization(name='bn1')(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "    \n",
    "    # Helper function for residual blocks\n",
    "    def residual_block(x, filters, stride=1, name=''):\n",
    "        shortcut = x\n",
    "        \n",
    "        x = Conv2D(filters, CONFIG['kernel_size'], strides=stride, padding='same', \n",
    "                   name=f'{name}_conv1')(x)\n",
    "        if CONFIG['use_batch_norm']:\n",
    "            x = BatchNormalization(name=f'{name}_bn1')(x)\n",
    "        x = Activation('relu', name=f'{name}_relu1')(x)\n",
    "        \n",
    "        x = Conv2D(filters, CONFIG['kernel_size'], strides=1, padding='same', \n",
    "                   name=f'{name}_conv2')(x)\n",
    "        if CONFIG['use_batch_norm']:\n",
    "            x = BatchNormalization(name=f'{name}_bn2')(x)\n",
    "        \n",
    "        if stride != 1 or shortcut.shape[-1] != filters:\n",
    "            shortcut = Conv2D(filters, (1, 1), strides=stride, padding='same',\n",
    "                             name=f'{name}_shortcut_conv')(shortcut)\n",
    "            if CONFIG['use_batch_norm']:\n",
    "                shortcut = BatchNormalization(name=f'{name}_shortcut_bn')(shortcut)\n",
    "        \n",
    "        x = Add(name=f'{name}_add')([x, shortcut])\n",
    "        x = Activation('relu', name=f'{name}_relu2')(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # Stage 1\n",
    "    x = residual_block(x, filters_base, stride=1, name='stage1_block1')\n",
    "    x = residual_block(x, filters_base, stride=1, name='stage1_block2')\n",
    "    if dropout_conv > 0:\n",
    "        x = Dropout(dropout_conv, name='dropout_stage1')(x)\n",
    "    \n",
    "    # Stage 2\n",
    "    x = residual_block(x, filters_base * 2, stride=2, name='stage2_block1')\n",
    "    x = residual_block(x, filters_base * 2, stride=1, name='stage2_block2')\n",
    "    if dropout_conv > 0:\n",
    "        x = Dropout(dropout_conv, name='dropout_stage2')(x)\n",
    "    \n",
    "    # Stage 3\n",
    "    x = residual_block(x, filters_base * 4, stride=2, name='stage3_block1')\n",
    "    x = residual_block(x, filters_base * 4, stride=1, name='stage3_block2')\n",
    "    if dropout_conv > 0:\n",
    "        x = Dropout(dropout_conv, name='dropout_stage3')(x)\n",
    "    \n",
    "    # Stage 4\n",
    "    x = residual_block(x, filters_base * 8, stride=2, name='stage4_block1')\n",
    "    x = residual_block(x, filters_base * 8, stride=1, name='stage4_block2')\n",
    "    if dropout_conv > 0:\n",
    "        x = Dropout(dropout_conv, name='dropout_stage4')(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    outputs = Dense(CONFIG['num_classes'], activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='ResNet18')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65ea14",
   "metadata": {},
   "source": [
    "**Tuner**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5800d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(model_builder, model_name, search_strategy='random', max_trials=10):\n",
    "    # Create tuner based on strategy\n",
    "    if search_strategy == 'random':\n",
    "        tuner = kt.RandomSearch(\n",
    "            model_builder,\n",
    "            objective=kt.Objective('val_accuracy', direction='max'),\n",
    "            max_trials=max_trials,\n",
    "            executions_per_trial=1,\n",
    "            directory=f'tuning_{model_name}',\n",
    "            project_name=f'{search_strategy}_search',\n",
    "            overwrite=True,\n",
    "            seed=CONFIG['random_seed']\n",
    "        )\n",
    "    \n",
    "    elif search_strategy == 'bayesian':\n",
    "        tuner = kt.BayesianOptimization(\n",
    "            model_builder,\n",
    "            objective=kt.Objective('val_accuracy', direction='max'),\n",
    "            max_trials=max_trials,\n",
    "            executions_per_trial=1,\n",
    "            directory=f'tuning_{model_name}',\n",
    "            project_name=f'{search_strategy}_search',\n",
    "            overwrite=True,\n",
    "            seed=CONFIG['random_seed']\n",
    "        )\n",
    "    \n",
    "    elif search_strategy == 'hyperband':\n",
    "        tuner = kt.Hyperband(\n",
    "            model_builder,\n",
    "            objective=kt.Objective('val_accuracy', direction='max'),\n",
    "            max_epochs=CONFIG['epochs'],\n",
    "            factor=3,\n",
    "            directory=f'tuning_{model_name}',\n",
    "            project_name=f'{search_strategy}_search',\n",
    "            overwrite=True,\n",
    "            seed=CONFIG['random_seed']\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"search_strategy must be 'random', 'bayesian', or 'hyperband'\")\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Start search\n",
    "    print(\"\\nStarting hyperparameter search...\")\n",
    "    tuner.search(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=CONFIG['epochs'],\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Get best hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BEST HYPERPARAMETERS FOR {model_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  filters_base: {best_hps.get('filters_base')}\")\n",
    "    print(f\"  dropout_conv: {best_hps.get('dropout_conv')}\")\n",
    "    print(f\"  dropout_dense: {best_hps.get('dropout_dense')}\")\n",
    "    print(f\"  learning_rate: {best_hps.get('learning_rate')}\")\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    \n",
    "    # Evaluate best model\n",
    "    print(f\"\\nEvaluating best model on validation set...\")\n",
    "    val_loss, val_acc, val_precision, val_recall = best_model.evaluate(\n",
    "        val_generator, verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBest Model Performance:\")\n",
    "    print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Precision: {val_precision:.4f}\")\n",
    "    print(f\"  Val Recall: {val_recall:.4f}\")\n",
    "    \n",
    "    best_model.save(f'best_{model_name}_{search_strategy}.keras')\n",
    "    print(f\"\\nBest model saved as 'best_{model_name}_{search_strategy}.keras'\")\n",
    "    \n",
    "    tuner.results_summary()\n",
    "    \n",
    "    return tuner, best_hps, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cd38e",
   "metadata": {},
   "source": [
    "**Tuned CNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89948e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_tuner, cnn_best_hps, cnn_best_model = tune(\n",
    "    build_cnn_tuner, \n",
    "    model_name='cnn', \n",
    "    search_strategy='random',\n",
    "    max_trials=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb401b04",
   "metadata": {},
   "source": [
    "**Tuned ResNet18**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68204744",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_tuner, resnet_best_hps, resnet_best_model = tune(\n",
    "    build_resnet18_tuner, \n",
    "    model_name='resnet18', \n",
    "    search_strategy='random',\n",
    "    max_trials=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
