{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d2c03f",
   "metadata": {},
   "source": [
    "### **Loading libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b528ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dropout, Dense, RandomFlip, RandomRotation, RandomZoom, Input, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e686bd",
   "metadata": {},
   "source": [
    "# **Data Preparation and Pre-processing**\n",
    "\n",
    "## **Data exploration (EDA)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e64dd8",
   "metadata": {},
   "source": [
    "**Class counts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f3eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"FER/train\"\n",
    "test_dir = \"FER/test\"\n",
    "emotions = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "\n",
    "# Count images per class\n",
    "def count_images(base_dir):\n",
    "    counts = {}\n",
    "    for emotion in emotions:\n",
    "        path = base_dir + \"/\" + emotion\n",
    "        counts[emotion] = len(os.listdir(path))\n",
    "    return counts\n",
    "\n",
    "train_counts = count_images(train_dir)\n",
    "test_counts = count_images(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd909e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54223ec0",
   "metadata": {},
   "source": [
    "**Class distribution visualization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].bar(train_counts.keys(), train_counts.values())\n",
    "axes[0].set_title('Training Set Distribution')\n",
    "axes[0].set_xlabel('Emotion')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1].bar(test_counts.keys(), test_counts.values())\n",
    "axes[1].set_title('Test Set Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Imbalance Ratio: \")\n",
    "for emotion in emotions:\n",
    "    ratio = train_counts[emotion] / sum(train_counts.values()) * 100\n",
    "    print(f\"{emotion}: {ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db0632",
   "metadata": {},
   "source": [
    "**Analyze image dimensions and properties**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfdfac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b773f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_properties(base_dir, sample_per_class=None):\n",
    "    results = {\n",
    "        'dimensions': [],\n",
    "        'color_modes': [],\n",
    "        'file_sizes': [],\n",
    "        'emotions': [],\n",
    "        'corrupted_files': [],\n",
    "        'file_paths': []\n",
    "    }\n",
    "    \n",
    "    for emotion in emotions:\n",
    "        emotion_path = base_dir + \"/\" + emotion.lower()\n",
    "        files = os.listdir(emotion_path)\n",
    "        \n",
    "        if sample_per_class:\n",
    "            files = files[:sample_per_class]\n",
    "        \n",
    "        print(f\"Analyzing {emotion}: {len(files)} images...\")\n",
    "        \n",
    "        for file in files:\n",
    "            img_path = emotion_path + \"/\" + file\n",
    "            \n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                \n",
    "                # check image size (width & length)\n",
    "                results['dimensions'].append(img.size)\n",
    "\n",
    "                # check image color channel\n",
    "                results['color_modes'].append(img.mode)\n",
    "                \n",
    "                results['file_sizes'].append(os.path.getsize(img_path))\n",
    "                results['emotions'].append(emotion)\n",
    "                results['file_paths'].append(img_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                results['corrupted_files'].append({\n",
    "                    'path': img_path,\n",
    "                    'emotion': emotion,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "                print(f\"Corrupted: {file} - {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d899de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING SET RESULTS\")\n",
    "train_properties = analyze_image_properties(train_dir)\n",
    "print(\"Corrupted Files:\", len(train_properties['corrupted_files']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb202cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST SET RESULTS\")\n",
    "test_properties = analyze_image_properties(test_dir)\n",
    "print(\"Corrupted Files:\", len(test_properties['corrupted_files']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aacfe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension\n",
    "train_dims = Counter(train_properties['dimensions'])\n",
    "test_dims = Counter(test_properties['dimensions'])\n",
    "\n",
    "# Colour channel\n",
    "train_modes = Counter(train_properties['color_modes'])\n",
    "test_modes = Counter(test_properties['color_modes'])\n",
    "\n",
    "# image file size\n",
    "train_sizes_kb = np.array(train_properties['file_sizes']) / 1024\n",
    "test_sizes_kb = np.array(test_properties['file_sizes']) / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining set unique dimensions:\")\n",
    "for dim, count in train_dims.most_common():\n",
    "    print(f\"  {dim[0]}x{dim[1]}: {count} images ({count/len(train_properties['dimensions'])*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcbfcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest set unique dimensions:\")\n",
    "for dim, count in test_dims.most_common():\n",
    "    print(f\"  {dim[0]}x{dim[1]}: {count} images ({count/len(test_properties['dimensions'])*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "dim_labels_train = [f\"{d[0]}x{d[1]}\" for d in train_dims.keys()]\n",
    "dim_counts_train = list(train_dims.values())\n",
    "axes[0].bar(dim_labels_train, dim_counts_train)\n",
    "axes[0].set_title('Training Set: Image Dimensions')\n",
    "axes[0].set_xlabel('Dimension (WxH)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "dim_labels_test = [f\"{d[0]}x{d[1]}\" for d in test_dims.keys()]\n",
    "dim_counts_test = list(test_dims.values())\n",
    "axes[1].bar(dim_labels_test, dim_counts_test, color='orange')\n",
    "axes[1].set_title('Test Set: Image Dimensions')\n",
    "axes[1].set_xlabel('Dimension (WxH)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('image_dimensions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f99997",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining set color modes:\")\n",
    "for mode, count in train_modes.items():\n",
    "    mode_name = \"Grayscale\" if mode == 'L' else \"RGB\" if mode == 'RGB' else mode\n",
    "    print(f\"  {mode_name} ({mode}): {count} images ({count/len(train_properties['color_modes'])*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest set color modes:\")\n",
    "for mode, count in test_modes.items():\n",
    "    mode_name = \"Grayscale\" if mode == 'L' else \"RGB\" if mode == 'RGB' else mode\n",
    "    print(f\"  {mode_name} ({mode}): {count} images ({count/len(test_properties['color_modes'])*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ae3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set file sizes (KB):\")\n",
    "print(f\"  Mean: {np.mean(train_sizes_kb):.2f} KB\")\n",
    "print(f\"  Median: {np.median(train_sizes_kb):.2f} KB\")\n",
    "print(f\"  Std Dev: {np.std(train_sizes_kb):.2f} KB\")\n",
    "print(f\"  Min: {np.min(train_sizes_kb):.2f} KB\")\n",
    "print(f\"  Max: {np.max(train_sizes_kb):.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set file sizes (KB):\")\n",
    "print(f\"  Mean: {np.mean(test_sizes_kb):.2f} KB\")\n",
    "print(f\"  Median: {np.median(test_sizes_kb):.2f} KB\")\n",
    "print(f\"  Min: {np.min(test_sizes_kb):.2f} KB\")\n",
    "print(f\"  Max: {np.max(test_sizes_kb):.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c833cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].hist(train_sizes_kb, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Training Set: File Size Distribution')\n",
    "axes[0].set_xlabel('File Size (KB)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(np.mean(train_sizes_kb), color='red', linestyle='--', label=f'Mean: {np.mean(train_sizes_kb):.2f} KB')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(test_sizes_kb, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_title('Test Set: File Size Distribution')\n",
    "axes[1].set_xlabel('File Size (KB)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(np.mean(test_sizes_kb), color='red', linestyle='--', label=f'Mean: {np.mean(test_sizes_kb):.2f} KB')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('file_sizes.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc4f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pixel_intensity(base_dir, sample_size=500):\n",
    "    intensities = []\n",
    "    \n",
    "    for emotion in emotions:\n",
    "        emotion_path = os.path.join(base_dir, emotion)\n",
    "        files = os.listdir(emotion_path)[:sample_size//len(emotions)]\n",
    "        \n",
    "        for file in files:\n",
    "            img_path = os.path.join(emotion_path, file)\n",
    "            try:\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    intensities.extend(img.flatten())\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return np.array(intensities)\n",
    "\n",
    "train_intensities = analyze_pixel_intensity(train_dir, sample_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315be218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pixel intensity statistics:\")\n",
    "print(f\"  Mean: {np.mean(train_intensities):.2f}\")\n",
    "print(f\"  Median: {np.median(train_intensities):.2f}\")\n",
    "print(f\"  Std Dev: {np.std(train_intensities):.2f}\")\n",
    "print(f\"  Min: {np.min(train_intensities)}\")\n",
    "print(f\"  Max: {np.max(train_intensities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(train_intensities, bins=256, range=(0, 255), edgecolor='black', alpha=0.7)\n",
    "plt.title('Pixel Intensity Distribution (Sample of 500 Training Images)')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(np.mean(train_intensities), color='red', linestyle='--', label=f'Mean: {np.mean(train_intensities):.2f}')\n",
    "plt.legend()\n",
    "plt.savefig('pixel_intensity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e24cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intensity_by_emotion(base_dir, sample_per_class=100):\n",
    "    emotion_intensities = {emotion: [] for emotion in emotions}\n",
    "\n",
    "    for emotion in emotions:\n",
    "        emotion_path = base_dir + \"/\" + emotion.lower()\n",
    "        files = os.listdir(emotion_path)[:sample_per_class]\n",
    "        \n",
    "        for file in files:\n",
    "            img_path = emotion_path + \"/\" + file\n",
    "            try:\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    emotion_intensities[emotion].append(np.mean(img))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return emotion_intensities\n",
    "\n",
    "emotion_intensity_data = intensity_by_emotion(train_dir, sample_per_class=200)\n",
    "\n",
    "for emotion in emotions:\n",
    "    mean_intensity = np.mean(emotion_intensity_data[emotion])\n",
    "    std_intensity = np.std(emotion_intensity_data[emotion])\n",
    "    print(f\"{emotion.capitalize():10s}: Mean = {mean_intensity:.2f}, Std = {std_intensity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot([emotion_intensity_data[e] for e in emotions], tick_labels=emotions)\n",
    "plt.title('Pixel Intensity Distribution by Emotion Class')\n",
    "plt.ylabel('Mean Pixel Intensity')\n",
    "plt.xlabel('Emotion')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.savefig('intensity_by_emotion.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dbcdf1",
   "metadata": {},
   "source": [
    "## **Detecting and removing dirty data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e44119ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76feb71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing training set...\n",
      "Analyzing test set...\n"
     ]
    }
   ],
   "source": [
    "# detect images that are too dark or too bright\n",
    "def detect_extreme_intensity_images(base_dir, min_threshold=20, max_threshold=235):\n",
    "    extreme_images = []\n",
    "    \n",
    "    for emotion in emotions:\n",
    "        emotion_path = base_dir + \"/\" + emotion.lower()\n",
    "        files = os.listdir(emotion_path)\n",
    "        \n",
    "        for file in files:\n",
    "            img_path = emotion_path + \"/\" + file\n",
    "            try:\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                if img is not None:\n",
    "                    mean_intensity = np.mean(img)\n",
    "                    \n",
    "                    if mean_intensity < min_threshold:\n",
    "                        extreme_images.append({\n",
    "                            'path': img_path,\n",
    "                            'emotion': emotion,\n",
    "                            'filename': file,\n",
    "                            'mean_intensity': mean_intensity,\n",
    "                            'issue': 'too_dark'\n",
    "                        })\n",
    "                    elif mean_intensity > max_threshold:\n",
    "                        extreme_images.append({\n",
    "                            'path': img_path,\n",
    "                            'emotion': emotion,\n",
    "                            'filename': file,\n",
    "                            'mean_intensity': mean_intensity,\n",
    "                            'issue': 'too_bright'\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"  Error reading {file}: {e}\")\n",
    "    \n",
    "    return extreme_images\n",
    "\n",
    "print(\"Analyzing training set...\")\n",
    "train_extreme = detect_extreme_intensity_images(train_dir, min_threshold=20, max_threshold=235)\n",
    "print(\"Analyzing test set...\")\n",
    "test_extreme = detect_extreme_intensity_images(test_dir, min_threshold=20, max_threshold=235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e011b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set by emotion:\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set by emotion:\")\n",
    "for emotion in emotions:\n",
    "    too_dark = sum(1 for img in train_extreme if img['emotion'] == emotion and img['issue'] == 'too_dark')\n",
    "    too_bright = sum(1 for img in train_extreme if img['emotion'] == emotion and img['issue'] == 'too_bright')\n",
    "    if too_dark > 0 or too_bright > 0:\n",
    "        print(f\"  {emotion.capitalize():10s}: {too_dark} too dark, {too_bright} too bright\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae516048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set by emotion:\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set by emotion:\")\n",
    "for emotion in emotions:\n",
    "    too_dark = sum(1 for img in test_extreme if img['emotion'] == emotion and img['issue'] == 'too_dark')\n",
    "    too_bright = sum(1 for img in test_extreme if img['emotion'] == emotion and img['issue'] == 'too_bright')\n",
    "    if too_dark > 0 or too_bright > 0:\n",
    "        print(f\"  {emotion.capitalize():10s}: {too_dark} too dark, {too_bright} too bright\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5340df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_extreme_images(extreme_images, n_samples=10, title=\"Extreme Intensity Images\"):\n",
    "    if len(extreme_images) == 0:\n",
    "        print(\"No extreme images to display\")\n",
    "        return\n",
    "    \n",
    "    sample_size = min(n_samples, len(extreme_images))\n",
    "    samples = np.random.choice(len(extreme_images), sample_size, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, idx in enumerate(samples):\n",
    "        img_data = extreme_images[idx]\n",
    "        img = cv2.imread(img_data['path'], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        axes[i].imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "        axes[i].set_title(f\"{img_data['emotion']}\\nMean: {img_data['mean_intensity']:.1f}\\n({img_data['issue']})\", fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('extreme_intensity_samples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "if len(train_extreme) > 0:\n",
    "    visualize_extreme_images(train_extreme, n_samples=10, title=\"Training Set: Extreme Intensity Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84948f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUARANTINING EXTREME IMAGES\n",
      "==================================================\n",
      "\n",
      "Moved 0 images to quarantine folder: FER/quarantine_extreme_train\n",
      "\n",
      "Moved 0 images to quarantine folder: FER/quarantine_extreme_test\n",
      "\n",
      "⚠️ RECOMMENDATION: Review quarantined images manually before permanently deleting\n"
     ]
    }
   ],
   "source": [
    "def quarantine_extreme_images(base_dir, extreme_images, quarantine_dir='FER/quarantine_extreme'):\n",
    "    os.makedirs(quarantine_dir, exist_ok=True)\n",
    "    os.makedirs((quarantine_dir + '/' + 'too_dark'), exist_ok=True)\n",
    "    os.makedirs((quarantine_dir + '/' + 'too_bright'), exist_ok=True)\n",
    "    \n",
    "    moved_count = 0\n",
    "    \n",
    "    for img_data in extreme_images:\n",
    "        src = img_data['path']\n",
    "        dst_folder = quarantine_dir + '/' + str(img_data['issue'])\n",
    "        dst = dst_folder + \"/\" + f\"{img_data['emotion']}_{img_data['filename']}\"\n",
    "        \n",
    "        try:\n",
    "            shutil.move(src, dst)\n",
    "            moved_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error moving {src}: {e}\")\n",
    "    \n",
    "    print(f\"\\nMoved {moved_count} images to quarantine folder: {quarantine_dir}\")\n",
    "    return moved_count\n",
    "\n",
    "# Quarantine extreme images\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"QUARANTINING EXTREME IMAGES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Uncomment to execute:\n",
    "train_moved = quarantine_extreme_images(train_dir, train_extreme, 'FER/quarantine_extreme_train')\n",
    "test_moved = quarantine_extreme_images(test_dir, test_extreme, 'FER/quarantine_extreme_test')\n",
    "\n",
    "print(\"\\n⚠️ RECOMMENDATION: Review quarantined images manually before permanently deleting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ee66c",
   "metadata": {},
   "source": [
    "# **Model Training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19bb614",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (48, 48)\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    ")\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    \"FER/train\",\n",
    "    target_size=img_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'FER/test',\n",
    "    target_size=img_size,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c091d1",
   "metadata": {},
   "source": [
    "### **Improve data pipeline (prefetch + cache)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcbd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# train_set = train_set.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "# test_set = test_set.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab4bdd1",
   "metadata": {},
   "source": [
    "### **Data Augmentation Layer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.1),\n",
    "    RandomZoom(0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee4258",
   "metadata": {},
   "source": [
    "number of classes / type of facial expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffafd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f1be8",
   "metadata": {},
   "source": [
    "### **2D CNN Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(48, 48, 1)))\n",
    "\n",
    "model.add(data_augmentation)\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e874e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=60,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36649c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_set)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
